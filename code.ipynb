{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtI7Z-fajx4V",
        "outputId": "eda91a94-82a7-4dca-b05b-cf1117ebf666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "w0oF1Qbbs0Us"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Navigate to the directory containing your file\n",
        "os.chdir('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpWQuYRetTna",
        "outputId": "beb41a1f-41cb-47d8-e7bb-4d02a3319a1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "def download_nltk_resources():\n",
        "    \"\"\"Download required NLTK resources\"\"\"\n",
        "    resources = ['stopwords', 'wordnet']\n",
        "    for resource in resources:\n",
        "        try:\n",
        "            nltk.download(resource, quiet=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading {resource}: {str(e)}\")\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and preprocess text without using punkt tokenizer\"\"\"\n",
        "    try:\n",
        "        # Convert to string and lowercase\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Remove special characters and extra whitespace\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "\n",
        "        # Simple word splitting (no need for NLTK tokenizer)\n",
        "        words = text.split()\n",
        "\n",
        "        # Remove stopwords\n",
        "        try:\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            words = [w for w in words if w not in stop_words]\n",
        "        except:\n",
        "            pass  # Continue even if stopwords fail\n",
        "\n",
        "        # Lemmatization\n",
        "        try:\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "            words = [lemmatizer.lemmatize(w) for w in words]\n",
        "        except:\n",
        "            pass  # Continue even if lemmatization fails\n",
        "\n",
        "        return ' '.join(words)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in clean_text: {str(e)}\")\n",
        "        return text  # Return original text if cleaning fails\n",
        "\n",
        "def main():\n",
        "    # Download NLTK resources at startup\n",
        "    download_nltk_resources()\n",
        "\n",
        "    print(\"Loading and preparing data...\")\n",
        "    # Rest of your code remains the same\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7tLCkNktseZ",
        "outputId": "1ad6f8ce-3a0a-44cf-c856-2533cff46c98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing data...\n",
            "Prediction: Ham\n",
            "Confidence: Spam: 0.01%, Ham: 99.99%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "def download_nltk_resources():\n",
        "    \"\"\"Download required NLTK resources\"\"\"\n",
        "    resources = ['stopwords', 'wordnet']\n",
        "    for resource in resources:\n",
        "        try:\n",
        "            nltk.download(resource, quiet=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading {resource}: {str(e)}\")\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and preprocess text without using punkt tokenizer\"\"\"\n",
        "    try:\n",
        "        # Convert to string and lowercase\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Remove special characters and extra whitespace\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "\n",
        "        # Simple word splitting (no need for NLTK tokenizer)\n",
        "        words = text.split()\n",
        "\n",
        "        # Remove stopwords\n",
        "        try:\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            words = [w for w in words if w not in stop_words]\n",
        "        except:\n",
        "            pass  # Continue even if stopwords fail\n",
        "\n",
        "        # Lemmatization\n",
        "        try:\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "            words = [lemmatizer.lemmatize(w) for w in words]\n",
        "        except:\n",
        "            pass  # Continue even if lemmatization fails\n",
        "\n",
        "        return ' '.join(words)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in clean_text: {str(e)}\")\n",
        "        return text  # Return original text if cleaning fails\n",
        "\n",
        "def main():\n",
        "    # Download NLTK resources at startup\n",
        "    download_nltk_resources()\n",
        "\n",
        "    print(\"Loading and preparing data...\")\n",
        "    # Rest of your code remains the same\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Code to load and use the saved model\n",
        "import pickle\n",
        "\n",
        "def load_model():\n",
        "    # Load the saved model and vectorizer\n",
        "    with open('/content/drive/MyDrive/data/spam_model.pkl', 'rb') as model_file:\n",
        "        model = pickle.load(model_file)\n",
        "\n",
        "    with open('/content/drive/MyDrive/data/tfidf_vectorizer.pkl', 'rb') as vectorizer_file:\n",
        "        vectorizer = pickle.load(vectorizer_file)\n",
        "\n",
        "    return model, vectorizer\n",
        "\n",
        "def predict_email(email_text, model, vectorizer):\n",
        "    # Clean the text\n",
        "    cleaned_text = clean_text(email_text)\n",
        "    # Transform the text\n",
        "    text_tfidf = vectorizer.transform([cleaned_text])\n",
        "    # Make prediction\n",
        "    prediction = model.predict(text_tfidf)\n",
        "    probability = model.predict_proba(text_tfidf)\n",
        "\n",
        "    return prediction[0], probability[0]\n",
        "\n",
        "# Example usage\n",
        "model, vectorizer = load_model()\n",
        "new_email = \"\"\"\n",
        "hey there - - life sounds horribly busy . i figured it was .\n",
        "don ' t worry about me - - i just thought we should get things moving - - since\n",
        "now i have officially been replaced , and it has been announced to my direct\n",
        "reports here in london . fernley ( not sure if you have spoken to him ) is\n",
        "anxious to put an announcement out to enron europe that phillip lord and i\n",
        "are leaving and that melissa will be stepping in to replace us .\n",
        "since phillip already has a place to go , it would be nice if we could at\n",
        "least say that i am going to north america energy ops , or something - - even\n",
        "if we don ' t have all of the details worked out .\n",
        "i am not sure if you want me to come over and meet some of your commercial \"\"\n",
        "customers \"\" - - - also - i have never met delainey ( i don ' t think ) - - but\n",
        "maybe knowing frevert well is enough . i am planning to come in november\n",
        "( first half ) to work on europe ' s plan and coordinate with brent , wes and the\n",
        "other groups under frevert , so maybe i can meet some people then . would love\n",
        "to discuss\n",
        "monday is fine - - as i am sure today will be another crazy day .\n",
        "i know how all of those reviews are . john sherriff put a o 5 per review per\n",
        "day penalty on any dept head who didn ' t have their reviews done by august\n",
        "15 th , and of course i was on holiday the week before that , so i had 2 days to\n",
        "do 10 reviews .\n",
        "hopefully i can help you with that direct report problem .\n",
        "if i don ' t talk to you today , then have a good weekend ,\n",
        "beth\n",
        "enron capital & trade resources corp .\n",
        "from : sally beck 13 / 10 / 2000 01 : 19\n",
        "to : beth apollo / lon / ect @ ect\n",
        "cc :\n",
        "subject : stuff\n",
        "crazy week - - 7 : 00 pm and i just finished a project for delainey that i got\n",
        "at noon today . cancelled everything on the calendar and have been immersed\n",
        "ever since . patti gave me your message with your home number around 5 : 30\n",
        "today , just as she was leaving - - apologized for not giving it to me\n",
        "earlier . however , given my immediate project , i don ' t think that i could\n",
        "have called until now . and at 1 : 00 am london time , i think that this e : mail\n",
        "is much better .\n",
        "i am still really crunched tomorrow morning . all of our mid - year reviews\n",
        "must be completed by the time we have our ena office of the chair staff\n",
        "meeting at 2 : 30 tomorrow . big push , after an over percentage completion rate\n",
        "of about 60 % at our ena meeting two weeks ago . some of those unfinished\n",
        "reviews were mine to give ( 16 direct reports is a killer ) . i wrote three\n",
        "last night after tyler went to bed , finished at 1 : 00 am and gave all three\n",
        "reviews this morning . i have 5 more to write tonight , and to deliver\n",
        "tomorrow morning to be at 100 % . i will try to call you on my way into the\n",
        "office tomorrow morning from the car . if i miss you then , it will be late\n",
        "afternoon houston time before i could call you . next week shapes up to be\n",
        "much better .\n",
        "i am definitely very interested in continuing our conversations . would love\n",
        "to have to over here and involved in operations . so i will either catch you\n",
        "from my car tomorrow morning , or we can talk on monday .\"\"\"\n",
        "prediction, probability = predict_email(new_email, model, vectorizer)\n",
        "print(f\"Prediction: {prediction}\")\n",
        "print(f\"Confidence: Spam: {probability[1]:.2%}, Ham: {probability[0]:.2%}\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}